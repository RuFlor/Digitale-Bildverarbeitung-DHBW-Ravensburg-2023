{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checker\n",
    "import generator\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print('Tensorflow version:', tf.__version__, '(Minimum expected 2.7.0)')\n",
    "\n",
    "# Laden von MNIST-Datenset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten normalisieren\n",
    "x_train_normalized = x_train / 255.0\n",
    "x_test_normalized = x_test / 255.0\n",
    "x_train = x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test_normalized.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorverarbeitung/ Preprocessing\n",
    "Im Gegensatz zum Standard-Netz wurde bei diesem neuronalen Netz eine Vorverarbeitung bzw. Preprocessing der Labels hinzugefügt. Dies hat folgenden Grund:\n",
    "Die Idee ist es, die Daten vor dem Training für das Modell so vorzubereiten, dass das Modell effizienter mit den Daten arbeiten kann. Um dies zu erreichen wurde das Label-Format in einen binären Vektor mit der Länge 10 umgewandelt. Die Funktion One-Hot-Encoding wandelt dafür die Ziffern der Original-Labels in Vektoren um, die nur an der Stelle, die der Ziffer entspricht, eine Eins haben und sonst nur Nullen enthalten. Beispiel:\n",
    "#\n",
    "•\tOriginal-Label: 8\n",
    "#\n",
    "•\tOne-Hot-Encoding: [0,0,0,0,0,0,0,0,1,0]\n",
    "#\n",
    "In dieser Form sind die Labels besser für die Verarbeitung durch das neuronale Netzwerk geeignet, was zur Optimierung des Netzwerks beiträgt, wie auch an den Ergebnissen deutlich beobachtet werden kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Encoding der Labels\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellstruktur und -architektur\n",
    "Des Weiteren wurde das neuronale Netz in der Architektur angepasst. Die Modelldefiniton zeigt den Aufbau des Netzwerks. Aus Gründen der Optimierung wurde das sequentielle Modell aus folgenden Schichten aufgebaut:\n",
    "#\n",
    "•\tConv2D-Schichten (Convolutional Layer):\n",
    "Die Schichten führen eine Faltung auf dem Eingangsbild durch, um Merkmale aus den zweidimensionalen Eingangsbildern zu erhalten. Mit der Anzahl der Filter (gewählt wurden 32 bzw. 64) kann eingestellt werden, wie viele Merkmale die Schicht aus den Bildern erxtrahiert und im Training lernt. Im Beispiel des MNIST-Datensatzes können z.B. Kanten (z.B. Eck der Sieben) oder Muster (z.B. Kringel der Sechs) erkannt werden. Das steigert die Leistung des Netzwerks bei der Erkennung der Ziffern, da diese Muster u.a. unabhängig von der Position im Bild wiedererkannt werden können. Die Aktivierung erfolgt über Rectified Linear Unit (ReLU).\n",
    "#\n",
    "•\tMaxPooling2D-Schichten:\n",
    "Die Schichten sind in Kombination mit Conv2D-Schichten sehr effektiv, weshalb sie im Anschluss an solch eine Schicht angewandt werden. Die MaxPooling2D-Schicht reduziert die Größe der Merkmals-Daten der Conv2D-Schicht. Das kann als eine Art Filter gesehen werden und bewirkt einerseits die Fokussierung auf wesentliche Merkmale. Andererseits führt es dazu, dass das Modell nicht auf spezifische Merkmale oder Rauschen trainiert. Außerdem unterstützt es auch die positionsunabhängige Erkennung von Merkmalen. Weiterer positiver Effekt dieser Schichten ist, dass die Reduzierung der Datenmenge die Berechnungslast verringert und das neuronale Netzwerk dadurch effizienter wird.\n",
    "#\n",
    "•\tFlatten-Schicht:\n",
    "Die Flatten-Schicht wandelt die Daten (mehrdimensional), die in diese Schicht gegeben werden, in einen eindimensionalen Vektor um. Dies ist für die Verarbeitung der nachfolgenden Dense-Schichten notwendig.\n",
    "#\n",
    "•\tDense-Schichten:\n",
    "Jedes Neuron der Dense-Schichten (gewählt wurden z.B. 256 Neuronen) ist mit allen Neuronen aus den vorherigen Schichten verbunden. In den Dense-Schichten geschieht der eigentliche Lern-/ Trainingsprozess. Die Dense-Schicht knüpft sozusagen die Synapsen zwischen den Neuronen, um eine Analogie zur Biologie herzustellen. Dies geschieht durch das Anpassen der Gewichtungen und Parameter der Verbindungen einzelner Neuronen. Dadurch kann das Modell komplexe Zusammenhänge in den Daten erfassen. Die Dense-Schichten werden mit der Rectified Linear Unit (ReLU) aktiviert.\n",
    "#\n",
    "•\tDropout-Schicht:\n",
    "Die Dropout-Schicht deaktiviert zufällig Neuronen. Dies hat den Effekt, dass das neuronale Netzwerk nicht zu stark auf die Trainingsdaten abgestimmt wird und lernt nicht zu stark auf bestimmte Neuronen angewiesen zu sein. Das soll die Robustheit gegenüber Ausfällen stärken und die Generalisierungsfähigkeit des Modells verbessern, sodass das Netz auch auf neuen Daten gut funktioniert. Mit dem Parameter stellt man die Wahrscheinlichkeit für ein Neuron ein, mit der das Neuron deaktiviert wird.\n",
    "#\n",
    "•\tDense-Schicht (Ausgangsschicht):\n",
    "Die Ausgangs-Dense-Schicht hat nur 10 Neuronen die den 10 Klassen (Ziffern) entsprechen. Die Ausgangsschicht ist für die Klassifizierung zuständig und wird mit Softmax-Funktion aktiviert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelldefinition\n",
    "marvin = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "marvin.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierung des Netzes\n",
    "Für das Optimieren des Netzes während des Trainings wurden verschiedene Verfahren gewählt:\n",
    "#\n",
    "•\tAls Optimierer wird der Adam-Optimizer verwendet. Warum habe ich mich für den Adam-Optimizer entschieden? \n",
    "#\n",
    "Der Adam-Optimierer verwendet das Verfahren des stochastischen Gradienten-Abstiegs. Das bietet den Vorteil, dass eine adaptive Lernrate für jedes Gewicht möglich. Das soll die Konvergenz des Modells optimieren, da Gewichte mit kleinen Gradienten schneller konvergieren als Gewichte mit großen Gradienten. Auch ist der Adam-Optimierer für große Datensätze geeignet und steigert damit die Effizienz des neuronalen Netzwerks. Die Lernrate des Optimierers wurde auf 0,001 eingestellt, da die Lernrate somit nicht zu groß, aber auch nicht zu klein ist. Eine kleine Lernrate lässt das Modell langsam, jedoch stabil und mit konstantem Tempo konvergieren. Eine große Lernrate lässt das Modell schnell konvergieren, allerdings kann es passieren, dass das Modell dadurch überschießt oder oszilliert. Aus diesem Grund wurde mit der Lernrate 0,001 ein Kompromiss gewählt, der gute Ergebnisse erzielt.\n",
    "#\n",
    "•\tAls Loss bzw. Verlustfunktion wurde CategoricalCrossentropy gewählt. Warum habe ich mich für diese Verlustfunktion entschieden?\n",
    "#\n",
    "Als Verlustfunktion wurde CategoricalCrossentropy gewählt, da die Verlustfunktion speziell für Klassifizierungen mit mehreren Klassen konzipiert ist. CategoricalCrossentropy ist effizient und bietet eine gute Softmax-Kompatibilität. Dadurch können Wahrscheinlichkeiten für Klassen interpretiert werden. Die zusätzliche Fokussierung auf die größte Wahrscheinlichkeit führt dazu, dass das Modell durch CategoricalCrossentropy in den Vorhersagen besser wird.\n",
    "#\n",
    "•\tAls Metriken wurden, zusätzlich zur Standard-Metrik Accuracy, noch Precision und Recall verwendet. Warum habe ich mehrere Metriken verwendet und warum genau diese?\n",
    "#\n",
    "Mehrere Metriken sind sinnvoll, um die Leistung des Modells detaillierter zu bewerten. Auch können durch den Einsatz mehrerer Metriken verschiedene Aspekte, die die Leistung des Netzwerks bestimmen, betrachtet werden. Auch für die Anpassung der Hyperparameter und die Optimierung des Modells ist es hilfreich mehr Informationen über das Modell und seine Leistung zu haben.\n",
    "Die Accuracy-Metrik gibt die Genauigkeit des Modells an. Hierfür wird nur der Prozentsatz der richtig klassifizierten Samples betrachtet.\n",
    "Durch die Precision-Metrik kann die Präzision der Vorhersagen des Modells bewertet werden. Die Precision-Metrik misst den Prozentsatz der korrekt positiven Vorhersagen im Vergleich zu allen tatsächlich positiven Vorhersagen.\n",
    "Die Recall-Metrik bezieht sich im Gegensatz zur Precision-Metrik auf die Sensitivität der Vorhersagen. Sie misst den Prozentsatz der korrekt positiven Vorhersagen im Vergleich zu allen tatsächlich positiven Instanzen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilieren des Modells\n",
    "marvin.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# TensorBoard für die Überwachung\n",
    "import datetime\n",
    "import os\n",
    "%load_ext tensorboard\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des Netzes\n",
    "Für das Training des Modells werden verschiedene Einstellungen gewählt.\n",
    "#\n",
    "•\tFür das Training wurden 10 Epochen gewählt. Warum habe ich genau 10 Epochen gewählt?\n",
    "#\n",
    "Aufgrund der Wahl der Hyperparameter und der Einstellung der Lernrate auf 0,001 ist es notwendig, dass das Modell mehrere Epochen trainiert. Wird die Zahl der Epochen verringert, kann das Modell nicht konvergieren und man erreicht schlechte Ergebnisse in Genauigkeit, Präzision und Sensitivität. Ebenso ist eine deutlich höhere Anzahl an Epochen nicht förderlich für die Leistung des Netzes, da das Training über viele Epochen ressourcenaufwändig ist und viel Zeit beansprucht. Außerdem sinken Trainingsfortschritt und Optimierung, die pro Epoche erzielt werden mit jeder weiteren Epoche, d.h. das Kosten-Nutzen-Verhältnis wird von Epoche zu Epoche schlechter. Um also die Effizienz des Modells nicht zu reduzieren, wurde die Zahl der Epochen für das Training auf 10 eingestellt. Mit einer Epochenzahl von 15 kann ebenfalls gut gearbeitet werden, wie weitere Untersuchungen gezeigt haben.\n",
    "#\n",
    "•\tDie Batch-Größe wurde für das Training auf 128 gesetzt. Warum habe ich diese Batch-Größe gewählt?\n",
    "#\n",
    "Die Batch-Größe ist mit 128 weder besonders groß noch besonders klein. Auch das liegt daran, dass hier eine Kompromiss-Lösung gewählt wurde, welche perfekt auf das Modell zugeschnitten ist. Da die Batch-Größe im mittleren Bereich liegt ist das Verhältnis aus Recheneffizienz und Speichereffizienz sehr gut. Ebenso ist die Batch-Größe auf das Zusammenspiel mit den Hyperparametern abgestimmt, sodass die Batchgröße bei gegebenen Hyperparametern eine schnellere Konvergenz und Anpassung der Gewichte pro Epoche ermöglicht. Aus diesem Grund wurde die Batch-Größe von 128 gewählt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelltraining\n",
    "marvin.fit(\n",
    "    x_train,\n",
    "    y_train_one_hot,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test_one_hot),\n",
    "    callbacks=[tensorboard_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell speichern\n",
    "model_name = 'marvin.h5'\n",
    "marvin.save(model_name, save_format='h5')\n",
    "print('Success! You saved Marvin as: ', model_name)\n",
    "\n",
    "model_name = 'marvin.h5' \n",
    "marvin_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "predictions = marvin_reloaded.predict([x_test])\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "pd.DataFrame(predictions)\n",
    "\n",
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
